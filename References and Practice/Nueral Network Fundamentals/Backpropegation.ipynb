{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Cost Function* is the mechanism in our neural network that measures the error in an output to the network relative to the expected value. The known input output pairs that we expect the network to produce are called *training data*; a network that optimizes for training data is a *supervised model*. At its most basic representation, it is the a simple difference between the expected and actual result.\n",
    "\n",
    "$C(x) = \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partial derivative of any single weight value can be determined via the extended chain rule.\n",
    "\n",
    "Questions:\n",
    "    \n",
    "    Since weights in previous layers effect outputs down the line, does that effect need to be considered when analyzing their contribution to the output layer?\n",
    "    \n",
    "    Since training data is meant to adjust the weight and bias values across all training samples, how do we structure a solution which takes into account changes to these values towards accuracy for multiple training data?\n",
    "    \n",
    "   For the simplest example, one training data point, we can assess the necessary changes by interpretting each weight and bias in the network into a gradient. More specifically, we look at the *cost function*, which measures the difference between the expectec and actual output, and form a vector of partial derivatives corrosponding to each weight (and bias?); this vector is called the *gradient*, and specifies the direction of steepest ascent - by going in the opposite direction the cost/error function is minimized.\n",
    "   \n",
    "   The challenge, as mentioned, is to perform this optimization for multiple training data rather than a single one. This is where *backpropegation* comes into play.\n",
    "    \n",
    "   The *cost function* or *error function* is of greater scope than simply the difference in expected output versus actual output for a single peice of training data; it is in fact the average error across the entire range of training data. This is a necessary expansion since adjusting the weights and biases to fit just a single peice of training data would not ensure that a network can generalize past what it is trained for - it must be trained on a wide variety of data. ;;;(the sum of all the node errors (all of which were made positive through squaring the difference in the error function)). For each \n",
    "   \n",
    "   With this in mind, my current understanding how the backpropegation method might work is that for every peice of training data, a gradient is added into a matrix of column vectors. At this point the cost or error function can be minimized by towards a local minimum by following components of each column vector (partial derivatives of the cost function for each weight/bias in each training instance) which share the same direction of descent. This could be achieved by taking the average of each row in the gradient matrix, and shifting that weight in the general network towards a minimum proportionally to said average.\n",
    "   \n",
    "   So far despite seeing multiple tutorials derive the method of finding the partial derivatives of each weight in the cost function, I have not yet understood how each the appropriate change in weight for is determined if we are working with multiple training data which may have partial derivatives for the same weight, in a different training cycle, which conflict with one another.\n",
    "   \n",
    "   *Backpropegation* refers to the process of finding the magnitude of change for the weights/biases of the previous layer that will make your target layer (starting with the output layer) closer to the desired output. For a single node in your target layer, the gradient of that node's error can be computed with each weight connecting the previous layer's nodes as components. Considering the other input nodes may require movement in different directions, a gradient for each node in your target layer and the wieghts of the node layer previous is computed as previously described. Once this set of gradients is determined, with each node in the target layer corrosponding to a scalar magnitude to shift the weight connections to optimize for that target node, for each weight these scalars are summed, so as to minimize each target node's error as much as possible. \n",
    "   \n",
    "   The result of this single step of backpropegation is that the final layer connection in the network is now optimized to the given training example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After further research, the confusion around backpropegation was largely cleared up. The error function is most simply defined in terms of the output layer, where some function quantifying the difference between an output node's actual output and expected output is employed. From here the backpropegation method can be used for each output node to determine the gradient for each node the layer previous - once this is done for each output node, the set of gradeints for each node in the previous layer is summed to get the net change to that weight.\n",
    "\n",
    "Where the standard challenge comes in is implenenting this method again, from the second to last layer, to the next previous one. The error function relies on node values that can be compared to ideal or target values, which only exist for the final, or output layer. Therefore, another set of gradients cannot be computed without an explicit error function for our hidden layer. Our hidden layer error function takes a node's contribution to each node in the output layer, multiplied by its respective error (the one on the node connecting to the final layer), one for each node in the layer after for which its connected, in this case the output, and sums them together. Each of those terms being summed is divided by all of the weights in the layer for which the error is being computed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropegation is the process of determining how to change the weights and biases in a network to most effectivly fit the training data. This explenation revolves around doing this whole process for a single peice of training data, since doing this for all your training data seperately and then employing one last method to generalize the changes across the whole network is how basic backpropegation is actually done.\n",
    "\n",
    "Backpropegation is an iterative process, meaning that the weight and biase changes can be determined for the entire network before those changes are actually made. For this reason, understanding the larger process can be achieved by understanding how to determine these changes for any single layer. This begins with determining error for the output layer, and less plainly, determining error on the hidden layers as well. Since determining this error is the first step for each layer, this explanation will start there.\n",
    "\n",
    "The error of the output of the network is simple enough, and can stand host to different functions. The error term should be a magnitude, that is, a positive real number,,, and each output node can have computed its error. A common choice for this error functions is the difference in expected vs. actual output squared. Once these values are computed, then for each output node, a gradient vector can be determined by comparing the rate of change of each weight to the rate of change of the error - quantifying how each weight connected to that output can be changed to decrease the error; this is the *gradient descent* part of the algorithm. \n",
    "\n",
    "The details to carry through the changing of weights are discussed later, so that now we can answer the question of how we are supposed to repeat this process for the hidden layers if there are no 'expected values' to compare with our hidden node values to quantify our error? We need a formula for determining error on hidden layer nodes, so that the weights connecting to the previous layer can be adjusted the same way as the weights connecting the final hidden layer to the output layer.\n",
    "\n",
    "This 'hidden layer node error' makes use of our already computed error terms. To compute the error for a node in the first hidden layer, we first observe what output nodes it feeds into. This hidden layer node contributes some proportion of error into each output it feeds into, so for each hidden node, the error is the sum consisting of all its weights, each multiplied by the already computed error in the output node that a weight feeds into; importantly, since for every node that our hidden layer node feeds into is also fed into by some number of other hidden nodes from the same layer, each term in the sum is divided by the weights that also feed into the output node, ensuring that the error contributed from  the hidden layer is properly proportioned with all the other hidden nodes feeding into that known, output node error.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial{E}}{\\partial{w_{ij}}}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\">Back-Propegation</h1>\n",
    "    \n",
    "The goal of backpropegation is to minimize the difference between the outputs that a network produces for training data, and the known or target outputs of the training data. This difference for a sinlge final layer node is given by, for this demonstration, the *squared error function*.\n",
    "\n",
    "$\\epsilon(t, y) = (t - y)^2$ \n",
    "\n",
    "Where:\n",
    "\n",
    "$t$ is the target output and,\n",
    "\n",
    "$y$ is the network output.\n",
    "\n",
    "In general, the full *cost* or *error* of the network for a single training example is given by the sum of every $\\epsilon(t_i, y_i)$, that is, every final layer output node error for the training example. This summed error function is the target of minimization:\n",
    "\n",
    "$E(T, Y) = \\sum_{n=0}^m(t_n - y_n)^2 = \\sum_{n=0}^m\\epsilon_n $\n",
    "\n",
    "\n",
    "\n",
    "Since the parameters of our network are the weights and biases, we want to determine the gradient of every weight and bias in the network with respect to The Error Function. The components of this vector will be given by:\n",
    "\n",
    "$\\frac{\\partial{E}}{\\partial{w_{jk}^{l}}}$\n",
    "\n",
    "or\n",
    "\n",
    "$\\frac{\\partial{E}}{\\partial{b_{j}^{l}}}$\n",
    "\n",
    "That is, the partial derivative describing how each weight or bias affects the growth of the cost function, with superscript $l$ denoting the layer. Before it is described how to determine the gradient for a single training example, it is useful to define some terms:\n",
    "\n",
    "${\\text{net}}_j^l = \\sum_{k=1}^{n}({o_{j}^{l-1}w^{l}_{jk}}) + b^l_j$\n",
    "\n",
    "That is, the weighted sums of all nodes in layer $l-1$ that feed into node $j$ in layer $l$. Naturally, the input of this sum  is fed into the activation function and becomes the final, or *output* value of node $j$ in layer $l$; this value is given by:\n",
    "\n",
    "$o_j^l = \\alpha(\\text{net}_j^l)$\n",
    "\n",
    "Where $\\alpha$ is the activation function, which is defined for convenience here as *the sigmoid function*:\n",
    "\n",
    "$\\alpha(z) = \\frac{1}{1+e^{-z}}$\n",
    "\n",
    "whose derivative is given by:\n",
    "\n",
    "$\\alpha'(z) = \\alpha(z)(1 - \\alpha(z))$\n",
    "\n",
    "With these extra terms the partial derivative of the *error* with respect to the weight or bias can be expanded via the chain rule:\n",
    "\n",
    "$\\frac{\\partial{E}}{\\partial{w_{jk}^{l}}} = \\frac{\\partial{E}}{\\partial{o_{j}^{l}}} \\frac{\\partial{o_j^l}}{\\partial{\\text{net}_j^l}} \\frac{\\partial{\\text{net}_j^l}}{\\partial{w_{jk}^{l}}}$\n",
    "\n",
    "\n",
    "$\\frac{\\partial{E}}{\\partial{b_{j}^{l}}} = \\frac{\\partial{E}}{\\partial{o_{j}^{l}}} \\frac{\\partial{o_j^l}}{\\partial{\\text{net}_j^l}} \\frac{\\partial{\\text{net}_j^l}}{\\partial{b_{j}^{l}}}$\n",
    "\n",
    "Breaking these down term by term, starting from the end:\n",
    "\n",
    "$\\frac{\\partial{\\text{net}_j^l}}{\\partial{w_{jk}^{l}}} = \\frac{\\sum_{k=1}^{n}({o_{j}^{l-1}w^{l}_{jk}) + b_j^l}}{\\partial{w_{jk}^{l}}} = o_{j}$\n",
    "\n",
    "$\\frac{\\partial{\\text{net}_j^l}}{\\partial{b_{j}^{l}}} = \\frac{\\sum_{k=1}^{n}({o_{j}^{l-1}w^{l}_{jk}) + b_j^l}}{\\partial{w_{jk}^{l}}} = 1$\n",
    "\n",
    "Middle term:\n",
    "\n",
    "$\\frac{\\partial{o_j^l}}{\\partial{\\text{net}_j^l}} = \\frac{\\alpha(\\text{net}^l_j)}{\\text{net}^l_j} = \\alpha'(\\text{net}^l_j) = \\alpha(\\text{net}^l_j)(1 - \\alpha(\\text{net}^l_j))$ \n",
    "\n",
    "For the first term, the partial derivative is straightforward if the output term $o_j$ is a member of the final layer $l$, as $o_j^l = y$, is the direct input to the error function:\n",
    "\n",
    "$\\frac{\\partial{E}}{\\partial{o_{j}^{l}}} = \\frac{\\partial{\\sum_{n=0}^m(t_n - y_n)^2}}{\\partial{y_i}}= \\frac{\\partial{(t_i - y_i)^2}}{\\partial{y_i}} = 2(y - t)$\n",
    "\n",
    "So for any weight or bias, $w_{jk}^l$ or $b_j^l$ i.e. those affecting the final output layer, the full expressions for the gradient of the *error function* with respect to those terms are given by:\n",
    "\n",
    "$\\frac{\\partial{E}}{\\partial{w_{jk}^{l}}} = 2(y - t)\\alpha(\\text{net}^l_j)(1 - \\alpha(\\text{net}^l_j)) o_j = $\n",
    "\n",
    "\n",
    "$\\frac{\\partial{E}}{\\partial{b_{j}^{l}}} = 2(y - t) \\alpha(\\text{net}^l_j)(1 - \\alpha(\\text{net}^l_j)) $\n",
    "\n",
    "When the weight or bias term of this in question is not one that directly affects the final layer, meaning it's connected to a hidden layer node, the term $\\frac{\\partial{E}}{\\partial{o_{j}^{l-1}}}$ must have more care taken to it, as the node said weight or bias feeds into: ${o_{j}^{l-1}}$, itself feeds into *multiple* final layer nodes, spreading its error across them. For now we consider just *layer* $l-1$ nodes. Bringing back the first term in the first receded layer, and noting the full *error function* sum:\n",
    "\n",
    "$\\frac{\\partial{E}}{\\partial{o_{j}^{l-1}}} = \\frac{\\partial{\\sum_{n=0}^m(t_n - y_n)^2}}{\\partial{o_{j}^{l-1}}}$\n",
    "\n",
    "Since $o_j^{l-1}$ now feeds into multiple $o_j^l$ nodes, each one affecting the total error, the partial derivative of the error with respect to node, $o_j^{l-1}$, will include all terms of the sum of $E$, that is, the final layer output nodes, which $o_j^{l-1}$ feeds into. \n",
    "\n",
    "Define $A$ as the indices for the nodes that $o_j^{l-1}$ feeds into:\n",
    "\n",
    "$A = \\{g, h, ... z\\}$ \n",
    "\n",
    "The partial derivative above, for which $o_j^{l-1}$ feeds into $o_a^l$, with $a \\in A$, will include as many terms as are in $A$, and since these the partial derivatives of the full *error function* with respect to the $o_j^{l-1}$ nodes and the error is a sum (the full error function is always a sum), the partial derivative above will also be a sum:\n",
    "\n",
    "$\\frac{\\partial{E}}{\\partial{o_{j}^{l-1}}} = \\sum_{a=A}\\frac{\\partial E}{\\partial o_j^{l-1}}$\n",
    "\n",
    "By again expanding by the chain rule, the sum can be properly evaluated.\n",
    "\n",
    "$\\sum_{a=A}\\frac{\\partial E}{\\partial o_j^{l-1}} = \\sum_{a=A}\\frac{\\partial E}{\\text{net}^l_a}\\frac{\\text{net}^l_a}{\\partial o_j^{l-1}} = \\sum_{a=A}\\frac{\\partial E}{\\partial o^{l}_a}\\frac{\\partial o^{l}_a}{\\text{net}^l_a}\\frac{\\text{net}^l_a}{\\partial o^{l-1}_j}$\n",
    "\n",
    "Each term in this expanded expression is now easily computed, and beautifully, the primary term will always be the *error with respect to the output of the current layer minus one*, will always be one less than the layer with the previously computed *error with the respect the the output* - meaning the weight gradient can now be determined for all weights in the network.\n",
    "\n",
    "The last two terms of this sum can be always determined no matter the layer, and the first term relies on having knowledge of the previous layer's $\\frac{\\partial E}{\\partial 0_j^{l+1}}$ term - the beauty of this first term in the expression is that it *is the same* as the compact partial derivative, meaning that the compact partial derivative can always be determined using outputs from the $l+1$ nodes calculated in the last error. This iterative computation is carried out through the whole network to determine the weight gradient for the entire function of the network.\n",
    "\n",
    "The change in each weight is expressed by the following, where $\\eta$ is the *learning rate* parameter.\n",
    "\n",
    "$\\Delta w_{ji} = -\\eta \\frac{\\partial E}{\\partial w_ji} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
